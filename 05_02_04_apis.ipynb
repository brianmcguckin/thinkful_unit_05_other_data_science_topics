{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of an API\n",
    "**API**: Application programming interface\n",
    "- Most 'big' sites (Twitter, Google, etc) have APIs to access their information\n",
    "    - Allows access to information without using webpages\n",
    "    - Can as the server to send only the specific information desired\n",
    "    - Speeds up scraping as well as minimizes server demand\n",
    "    - Typically includes throttling by limiting number of server requests per hour\n",
    "- Access: request a key that program provides with each API call\n",
    "    - API key (or token) uniquely identifies you\n",
    "    - Lets the API provider monitor your usage\n",
    "    - Security measure:keys can have different levels of authorization/access\n",
    "    - Can be set to expire after a certain amount of time or number of uses\n",
    "- Requests: program requests the data with a call to the API, including...\n",
    "    - Method: type of query made using language defined by the API\n",
    "    - Parameters: refine the query\n",
    "- Response: data returned by API, typically in a common format like json\n",
    "\n",
    "### Basics of API Queries: Wikipedia's API\n",
    "- Wikipedia's API doesn't require an authorization key\n",
    "    - When required, since scrapy can handle authorization so it can be used to access APIs\n",
    "- Goal: use [Wikipedia's API](https://www.mediawiki.org/wiki/API:Main_page) to get what other entries on Wikipedia link to the Monty Python page\n",
    "    - To do this by scraping, would have to scrape every single page on Wikipedia (very inefficient)\n",
    "    - To accomplish this, can build a query using the [Wikipedia API Sandbox](https://en.wikipedia.org/wiki/Special:ApiSandbox)\n",
    "    - Query is: `https://en.wikipedia.org/\n",
    "    w/api.phpaction=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect`\n",
    "    - Broken down:\n",
    "        - `w/api.php`: tells the server we are using an API rather than scraping raw pages\n",
    "        - `action=query`: want information from the API (as opposed to changing information in the API)\n",
    "        - `format=xml`: return format in xml, then we will parse with xpath\n",
    "        - `prop=linkshere`: we are interested in which pages link to target page\n",
    "        - `titles=Monty_Python`: setting target page using exact page name\n",
    "        - `lhprop=title`: from those links, want the title of each page\n",
    "        - `redirect`: also want to know if the link is a redirect\n",
    "        \n",
    "### Using Scrapy for API calls\n",
    "- If query can be answered in one response, scrapy is overkill\n",
    "    - Can just use requests library to make call and library like lxml to parse the return\n",
    "- Wikipedia's API will only return ten items at a time in response to a query (to avoid overwhelming the server)\n",
    "- Can use scrapy to iterate over query results (same way iterate over pages when scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = 'WS'\n",
    "    \n",
    "    # insert API call\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # identify the information wanted from the query response\n",
    "    # and extract it with xpath\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # the ns code identifies the type of page the link comes from\n",
    "            # '0' means it is a Wikipedia entry.\n",
    "            # other codes indicate links from 'Talk' pages, etc\n",
    "            # only interested in wikipedia entries, filter for only '0':\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # information necessary to get the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # recursively call spider to process next ten entries, if they exist\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # robots.txt file doesn't apply since using API queries\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # CLOSESPIDER_PAGECOUNT to limit scraper to the first 100 links    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# start the crawler with spider\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('first 100 links extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "monty=pd.read_json('PythonLinks.json', orient='records')\n",
    "print(monty.shape)\n",
    "print(monty.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API call was successful, saved 94 out of 100 (6 aren't links from entry pages).\n",
    "- Authorization keys\n",
    "    - Often simply included in the query string as an argument\n",
    "    - If need to enter key or login to form, scrapy has this functionality\n",
    "\n",
    "### Challenge:\n",
    "Pick a different website and write a scraper that will:\n",
    "- Return specific pieces of information (rather than just downloading a whole page)\n",
    "- Iterate over multiple pages/queries\n",
    "- Save the data to your computer\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class mapsSpider(scrapy.Spider):\n",
    "    name = 'mapsSpider'\n",
    "    # API call\n",
    "    # maps api key AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0\n",
    "    start_urls = ['https://maps.googleapis.com/maps/api/place/textsearch/xml?query=restaurants+near+wrigley+field&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0']\n",
    "    \n",
    "    # extract first 10 restuarants near wrigley field\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//result'):\n",
    "            yield {\n",
    "                'name': item.xpath('name/text()').extract_first(),\n",
    "                'address': item.xpath('formatted_address/text()').extract_first(),\n",
    "                    }\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'mapsResults.json',\n",
    "    # robots.txt file doesn't apply since using API queries\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # CLOSESPIDER_PAGECOUNT to limit scraper to the first 100 links    \n",
    "    #'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "\n",
    "process.crawl(mapsSpider)\n",
    "process.start()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3456 N Sheffield Ave, Chicago, IL 60657, USA</td>\n",
       "      <td>Cozy Noodles n' Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3463 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Dimo's Pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3664 N Clark St, Chicago, IL 60613, USA</td>\n",
       "      <td>Bernie's Tap &amp; Grill (Across from Wrigley Field)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3908 N Sheridan Rd, Chicago, IL 60613, USA</td>\n",
       "      <td>PR Italian Bistro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3343 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Lowcountry Lakeview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3731 N Clark St, Chicago, IL 60613, USA</td>\n",
       "      <td>Azteca Grill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3800 N Clark St, Chicago, IL 60613, USA</td>\n",
       "      <td>Uncommon Ground (Lakeview)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1017 W Irving Park Rd, Chicago, IL 60613, USA</td>\n",
       "      <td>Byron's Hotdogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1011 W Irving Park Rd, Chicago, IL 60613, USA</td>\n",
       "      <td>Tac Quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>936 W Addison St, Chicago, IL 60613, USA</td>\n",
       "      <td>El Burrito Mexicano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3472 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Lucky's Sandwich Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3716 N Clark St, Chicago, IL 60613, USA</td>\n",
       "      <td>Big G's Pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3506 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Old Crow Smokehouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3763 N Southport Ave, Chicago, IL 60613, USA</td>\n",
       "      <td>Tango Sur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3311 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Mia Francesca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3481 N Clark St, Chicago, IL 60657, USA</td>\n",
       "      <td>Nola Pub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3435 N Sheffield Ave, Chicago, IL 60657, USA</td>\n",
       "      <td>Rice'N Bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3601 N Southport Ave, Chicago, IL 60613, USA</td>\n",
       "      <td>Julius Meinl Restaurant and Cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3411 N Halsted St, Chicago, IL 60657, USA</td>\n",
       "      <td>Chicago Diner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3404 N Halsted St, Chicago, IL 60657, USA</td>\n",
       "      <td>HB Home Bistro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          address  \\\n",
       "0    3456 N Sheffield Ave, Chicago, IL 60657, USA   \n",
       "1         3463 N Clark St, Chicago, IL 60657, USA   \n",
       "2         3664 N Clark St, Chicago, IL 60613, USA   \n",
       "3      3908 N Sheridan Rd, Chicago, IL 60613, USA   \n",
       "4         3343 N Clark St, Chicago, IL 60657, USA   \n",
       "5         3731 N Clark St, Chicago, IL 60613, USA   \n",
       "6         3800 N Clark St, Chicago, IL 60613, USA   \n",
       "7   1017 W Irving Park Rd, Chicago, IL 60613, USA   \n",
       "8   1011 W Irving Park Rd, Chicago, IL 60613, USA   \n",
       "9        936 W Addison St, Chicago, IL 60613, USA   \n",
       "10        3472 N Clark St, Chicago, IL 60657, USA   \n",
       "11        3716 N Clark St, Chicago, IL 60613, USA   \n",
       "12        3506 N Clark St, Chicago, IL 60657, USA   \n",
       "13   3763 N Southport Ave, Chicago, IL 60613, USA   \n",
       "14        3311 N Clark St, Chicago, IL 60657, USA   \n",
       "15        3481 N Clark St, Chicago, IL 60657, USA   \n",
       "16   3435 N Sheffield Ave, Chicago, IL 60657, USA   \n",
       "17   3601 N Southport Ave, Chicago, IL 60613, USA   \n",
       "18      3411 N Halsted St, Chicago, IL 60657, USA   \n",
       "19      3404 N Halsted St, Chicago, IL 60657, USA   \n",
       "\n",
       "                                                name  \n",
       "0                               Cozy Noodles n' Rice  \n",
       "1                                       Dimo's Pizza  \n",
       "2   Bernie's Tap & Grill (Across from Wrigley Field)  \n",
       "3                                  PR Italian Bistro  \n",
       "4                                Lowcountry Lakeview  \n",
       "5                                       Azteca Grill  \n",
       "6                         Uncommon Ground (Lakeview)  \n",
       "7                                    Byron's Hotdogs  \n",
       "8                                          Tac Quick  \n",
       "9                                El Burrito Mexicano  \n",
       "10                              Lucky's Sandwich Co.  \n",
       "11                                     Big G's Pizza  \n",
       "12                               Old Crow Smokehouse  \n",
       "13                                         Tango Sur  \n",
       "14                                     Mia Francesca  \n",
       "15                                          Nola Pub  \n",
       "16                                      Rice'N Bread  \n",
       "17                  Julius Meinl Restaurant and Cafe  \n",
       "18                                     Chicago Diner  \n",
       "19                                    HB Home Bistro  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_json('mapsResults.json', orient='records')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVERYTHING ABOVE THIS CELL WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class mapsNearbySpider(scrapy.Spider):\n",
    "    name = 'mapsNearbySpider'\n",
    "    \n",
    "    def __init__(self, queryStr, nearbyStr):\n",
    "        super(mapsNearbySpider, self).__init__(queryStr, nearbyStr)\n",
    "        self.start_urls = ['https://maps.googleapis.com/maps/api/place/textsearch/xml?{}&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'.format(queryStr + '+near+' + nearbyStr)]\n",
    "        #self.queryTerm = queryStr\n",
    "        #self.nearbyTerm = nearbyStr\n",
    "        \n",
    "        #url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/xml?'\n",
    "        #url_key = '&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'\n",
    "        #url_args = queryStr + '+near+' + nearbyStr\n",
    "        #url_complete = url_base + argsQuery + url_key\n",
    "    \n",
    "    # build api call\n",
    "    # maps api key AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0\n",
    "    #url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/xml?'\n",
    "    #url_key = '&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'\n",
    "    #url_args = queryStr + '+near+' + nearbyStr\n",
    "    #url_complete = url_base + argsQuery + url_key\n",
    "    \n",
    "    #self.\n",
    "    #start_urls = ['https://maps.googleapis.com/maps/api/place/textsearch/xml?query=restaurants+near+wrigley+field&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0']\n",
    "    \n",
    "    # extract first 10 restuarants near wrigley field\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//result'):\n",
    "            yield {\n",
    "                'name': item.xpath('name/text()').extract_first(),\n",
    "                'address': item.xpath('formatted_address/text()').extract_first(),\n",
    "                    }\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'mapsResults.json',\n",
    "    # robots.txt file doesn't apply since using API queries\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # CLOSESPIDER_PAGECOUNT to limit scraper to the first 100 links    \n",
    "    #'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "\n",
    "wrigley_spider = mapsNearbySpider('restuarants', 'wrigley+field')\n",
    "\n",
    "process.crawl(wrigley_spider)\n",
    "process.start()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/xml?'\n",
    "argsQuery = 'query=restaurants+near+wrigley+field'\n",
    "url_key = '&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'\n",
    "url_complete = url_base + argsQuery + url_key\n",
    "url_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTerm = 'restaurants'\n",
    "nearbyTerm = 'wrigley' + '+' + 'field'\n",
    "nearbyTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_constructor = queryTerm + '+near+' + nearbyTerm\n",
    "query_constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/xml?'\n",
    "argsQuery = 'query=' + query_constructor\n",
    "url_key = '&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'\n",
    "url_complete = str(url_base + argsQuery + url_key)\n",
    "print('{}'.format(url_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class mapsNearbySpider(scrapy.Spider):\n",
    "    name = 'mapsNearbySpider'\n",
    "\n",
    "    def __init__(self, queryStr, nearbyStr):\n",
    "        self.queryStr = queryStr\n",
    "        self.nearbyStr = nearbyStr\n",
    "    \n",
    "    # build api call\n",
    "    def build_url(self):\n",
    "        url_base = 'https://maps.googleapis.com/maps/api/place/textsearch/xml?'\n",
    "        url_key = '&key=AIzaSyDbSk0561WqHmDUagcZqTzDvzTHd6ol7i0'\n",
    "        url_args = self.queryStr + '+near+' + self.nearbyStr\n",
    "        url_complete = url_base + argsQuery + url_key\n",
    "        return url_complete\n",
    "    \n",
    "    # make api call\n",
    "    def api_call(self):\n",
    "        self.build_url()\n",
    "        start_urls = [url_complete]\n",
    "    \n",
    "    # extract name & address from response\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//result'):\n",
    "            yield {\n",
    "                'name': item.xpath('name/text()').extract_first(),\n",
    "                'address': item.xpath('formatted_address/text()').extract_first(),\n",
    "                    }\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'mapsResults.json',\n",
    "    # robots.txt file doesn't apply since using API queries\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # CLOSESPIDER_PAGECOUNT to limit scraper to the first 100 links    \n",
    "    #'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "\n",
    "process.crawl(mapsNearbySpider('restaurants', 'wrigley+field'))\n",
    "process.start()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = mapsTest('restaurants', 'wrigley+field')\n",
    "test_class.build_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mapsTest():\n",
    "\n",
    "    def __init__(self, queryStr, nearbyStr):\n",
    "        self.queryStr = queryStr\n",
    "        self.nearbyStr = nearbyStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
