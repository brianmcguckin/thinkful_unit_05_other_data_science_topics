{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Big Data\n",
    "### How big is big data\n",
    "- Rule of thumb: the more experience someone has working with data/modern techniques, the larger a dataset has to be to qualify as 'big data'\n",
    "- **Big data** for our purposes is data that is too large to work with on a local machine\n",
    "    - Not to mean databases that are too large to clone to a local machine\n",
    "    - Rather, the only the data desired to work with from database is too large to work with locally\n",
    "- At least gigabtyes of data:\n",
    "    - Gigabytes: millions of rows\n",
    "    - Terabytes: billions of rows\n",
    "    - Petabytes/exabytes: most likely require specialized machines\n",
    "### Issues of big data\n",
    "- Accessing big data: methods previously used won't work, including single job sql\n",
    "- Data science vs. data engineering: not all companies (esp. smaller ones) differentiates these the same way or at all\n",
    "    - Engineering covers designing a storage system\n",
    "    - Science is concerned with accessing, manipulating, analyzing, and interpreting\n",
    "- Understanding big data: visualization and summary statistics still apply, but implementing them can change\n",
    "    - Outliers: could be thousands or more, not just a few\n",
    "    - Visualization: keep in mind...\n",
    "        - Visualizations are simplifications, they inherently gloss over some data\n",
    "        - Analysis is potentially more impacted as the amount of 'missed' data increases\n",
    "- Training: there are huge advantages to training models on big data\n",
    "    - Some models (eg NNs) require large amounts of data to perform well\n",
    "    - But:\n",
    "        - Can't work with data locally\n",
    "        - May be too slow to work with all at once for modeling purposes\n",
    "        - Need to have methods for efficiently working with big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Hadoop and Big Data Storage\n",
    "- Hadoop for our purposes refers to the larger environment of Hadoop-based software used for storing, moving, and analyzing big data under a unified framework\n",
    "- Components of Hadoop discussed here are only part of the picture\n",
    "    - Focus here is on the parts that are useful for model building and analytics\n",
    "    - Much of the infrastructural backend won't be covered\n",
    "### Key Components\n",
    "- Hadoop (or Hadoop clusters) have four core components\n",
    "1. Commons: utilities structure; typically handled by engineering\n",
    "2. YARN: scheduling and resourcing tool\n",
    "3. HDFS (Hadoop distributed file system): distributed data store with fast access tools\n",
    "    - Data is distributed across many machines/drives instead of a single one\n",
    "    - Advantages include speed of access & stability (not dependent on a single machine)\n",
    "4. MapReduce: data processing tool for distributed systems\n",
    "    - Basis of the Hadoop project as a whole\n",
    "    - PIG: tool for pulling raw data from HDFS\n",
    "        - Functionally equivalent to MapReduce with more intuitive querying based interface\n",
    "        - Navigating data can still be a challenge, but this is set by the internal data model & structures\n",
    "### Other Pieces\n",
    "- Data scientists most often won't be working with these tools (except PIG)\n",
    "- Data is most often set up with a querying layer\n",
    "    - Hive is most common for Hadoop\n",
    "    - Could also be Presto or a PIG Script\n",
    "- Hive: allows for the use of sql type tools/structures for large datasets in HDFS\n",
    "    - Speed: HiveQL is slower, partly because of data size but also because its generally slow\n",
    "        - Hive queries can take hours or days\n",
    "        - Faster tools (Presto) / database types (Redshift, Vertica) exist but are costly and require more hardware\n",
    "    - Syntax: slightly different around joins and datetimes (which is the case between sql languages as well)\n",
    "    - Other differences are minor from a user perspective\n",
    "        - Most differences are engineering related, stemming from the nature of working on distributed database\n",
    "        - working with distributed data works and feels similar to local work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Distributed Computing and Spark\n",
    "- Using standard tools on big data translates to slow analysis & modeling\n",
    "- Big data also has its own tools/techniques\n",
    "\n",
    "### Multicore Computing\n",
    "- **Parallelization**: packages like sklearn have n_jobs option to set the number of cores to utilize when training model\n",
    "    - Trains on multiple cores simultaneously\n",
    "    - Theoretically, processing time is divided by the number of cores (2 cores over 1 = half the time to train)\n",
    "- Some models are easier to parallelize than others but most can be parallelized in some way\n",
    "    - Random forest: different cores generate different trees\n",
    "    - Boosted trees: as trees split, subsequent models run in different cores\n",
    "    - SVM doesn't parallelize well, uses memory for points near margins\n",
    "\n",
    "### SPARK\n",
    "- Training/running models locally is often out of the question\n",
    "    - Too much data\n",
    "    - Too long to train\n",
    "- Spark: distributed computing tool from the Apache suite built up around hadoop\n",
    "    - PySpark: python like syntax for Spark making it easy to translate python to spark\n",
    "        - Looks nearly identical to python\n",
    "        - Requires infrastructure set up to run\n",
    "    - There also Spark versions of iPython/Jupyter notebooks and SKLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Where's the code?\n",
    "- No code has been written this section for three reasons:\n",
    "    1. Functionally, the code to build these systems is generally outside the scope of data science\n",
    "    2. Hundreds of cores/drives required to use Hadoop or Spark as intended\n",
    "        - Typically using cloud computing like AWS\n",
    "    3. Data science is more concerned with using these systems rather than architecting them\n",
    "        - Stacks are largely unstandardized and have a large range of possible tools and implementations\n",
    "        - Until working with a specific big data implementation, focus on understanding why the tools exists and what they do\n",
    "        - Learn to use specific tools as needed rather than abstractly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
